{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "from functools import partial\n",
    "import dataclasses\n",
    "import serde\n",
    "\n",
    "import jax\n",
    "import optax\n",
    "from jax import random, numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "\n",
    "@serde.serde\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    num_heads: int\n",
    "    embed_dim: int\n",
    "    num_hidden_layers: int\n",
    "    max_turns: int = 300\n",
    "\n",
    "    def create_model(self) -> 'Transformer':\n",
    "        return Transformer(self)\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    embed_dim: int\n",
    "    num_cards: int = 53\n",
    "    num_players: int = 5\n",
    "    max_turns: int = 300\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray, eval: bool):\n",
    "        embeddings = jnp.zeros((*x.shape[:-1], self.embed_dim))\n",
    "\n",
    "        card_embed = nn.Embed(self.num_cards, self.embed_dim)\n",
    "\n",
    "        for i in range(7):\n",
    "            embeddings += card_embed(x[..., i])\n",
    "\n",
    "        embeddings += nn.Embed(self.num_players, self.embed_dim)(x[..., 7])\n",
    "        embeddings += nn.Embed(2, self.embed_dim)(x[..., 8])\n",
    "        embeddings += nn.Embed(self.max_turns, self.embed_dim)(x[..., 9])\n",
    "\n",
    "        embeddings = nn.LayerNorm(epsilon=1e-12)(embeddings)\n",
    "        embeddings = nn.Dropout(0.5, deterministic=eval)(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    num_heads: int\n",
    "    embed_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, mask):\n",
    "        seq_len = x.shape[1]\n",
    "        head_dim = self.embed_dim // self.num_heads\n",
    "\n",
    "        v = nn.Dense(features=self.embed_dim)(x)  # [Batch, SeqLen, Head * Dim]\n",
    "        q = nn.Dense(features=self.embed_dim)(x)  # [Batch, SeqLen, Head * Dim]\n",
    "        k = nn.Dense(features=self.embed_dim)(x)  # [Batch, SeqLen, Head * Dim]\n",
    "\n",
    "        v = v.reshape(-1, seq_len, self.num_heads, head_dim)  # [Batch, SeqLen, Head, Dim]\n",
    "        q = q.reshape(-1, seq_len, self.num_heads, head_dim)  # [Batch, SeqLen, Head, Dim]\n",
    "        k = k.reshape(-1, seq_len, self.num_heads, head_dim)  # [Batch, SeqLen, Head, Dim]\n",
    "\n",
    "        # [Batch, Head, SeqLen, SeqLen]\n",
    "        attention = (jnp.einsum('...qhd,...khd->...hqk', q, k) / jnp.sqrt(head_dim))\n",
    "\n",
    "        attention = jnp.where(mask, attention, -jnp.inf)\n",
    "        attention = nn.softmax(attention, axis=-1)\n",
    "\n",
    "        values = jnp.einsum('...hqk,...khd->...qhd', attention, v)  # [Batch, SeqLen, Head, Dim]\n",
    "        values = values.reshape(-1, seq_len, self.num_heads * head_dim)  # [Batch, SeqLen, Head Ã— Dim (=EmbedDim)]\n",
    "        out = nn.Dense(self.embed_dim)(values)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    embed_dim: int\n",
    "    intermediate_size: int = 128\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, eval):\n",
    "        x = nn.Dense(features=self.embed_dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.embed_dim)(x)\n",
    "        x = nn.Dropout(0.1, deterministic=eval)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    num_heads: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.attention = MultiHeadAttention(self.num_heads, self.embed_dim)\n",
    "        self.feed_forward = FeedForward(embed_dim=self.embed_dim)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, attention_mask, eval):\n",
    "        out = self.attention(x, attention_mask)\n",
    "\n",
    "        x = x + out\n",
    "        x = nn.LayerNorm()(x)\n",
    "        x = x + self.feed_forward(x, eval)\n",
    "        x = nn.LayerNorm()(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    config: TransformerConfig\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.config)\n",
    "\n",
    "    def setup(self):\n",
    "        self.embeddings = Embeddings(self.config.embed_dim)\n",
    "        self.st_dence = nn.Dense(features=self.config.embed_dim)\n",
    "\n",
    "        self.layers = [\n",
    "            TransformerBlock(self.config.num_heads, self.config.embed_dim)\n",
    "            for _ in range(self.config.num_hidden_layers)\n",
    "        ]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray, eval=True):\n",
    "        x = self.embeddings(x, eval)\n",
    "\n",
    "        # [Batch, 1, SeqLen, SeqLen]\n",
    "        mask = nn.make_causal_mask(jnp.zeros((x.shape[0], x.shape[1])), dtype=bool)\n",
    "\n",
    "        for i in range(self.config.num_hidden_layers):\n",
    "            x = self.layers[i](x, mask, eval=eval)\n",
    "\n",
    "        x = nn.Dropout(0.1, deterministic=eval)(x)\n",
    "\n",
    "        p = nn.Dense(features=32, name=\"head_p\")(x)\n",
    "        v = nn.Dense(features=7, name=\"head_v\")(x)\n",
    "        c = nn.Dense(features=8, name=\"head_c\")(x)\n",
    "\n",
    "        return p, v, c  # [Batch, SeqLen, ...]\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def calc_loss(\n",
    "    x: jnp.ndarray,\n",
    "    a_pred: jnp.ndarray,\n",
    "    a_true: jnp.ndarray,\n",
    ") -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "\n",
    "    mask = jnp.any(x != 0, axis=-1)\n",
    "    mask = mask.reshape(-1)\n",
    "\n",
    "    # [Batch, SeqLen, 144]\n",
    "    p_true = p_true.reshape(-1)\n",
    "    v_true = jnp.stack([v_true]*v_pred.shape[-2], axis=-1).reshape(-1)\n",
    "    c_true = jnp.stack([c_true]*c_pred.shape[-2], axis=-1).reshape(-1, 8)\n",
    "    # c_true = c_true.reshape(-1, 1, 8)\n",
    "\n",
    "    p_pred = p_pred.reshape(-1, 32)\n",
    "    v_pred = v_pred.reshape(-1, 7)\n",
    "    c_pred = c_pred.reshape(-1, 8)\n",
    "\n",
    "    loss_p = optax.softmax_cross_entropy_with_integer_labels(p_pred, p_true)\n",
    "    loss_v = optax.softmax_cross_entropy_with_integer_labels(v_pred, v_true)\n",
    "    loss_c = optax.sigmoid_binary_cross_entropy(c_pred, c_true).mean(axis=-1)\n",
    "\n",
    "    loss_p = jnp.average(loss_p, weights=mask)\n",
    "    loss_v = jnp.average(loss_v, weights=mask)\n",
    "    loss_c = jnp.average(loss_c, weights=mask)\n",
    "\n",
    "    loss = loss_p + loss_v + loss_c\n",
    "    losses = jnp.array([loss_p, loss_v, loss_c])\n",
    "\n",
    "    return loss, losses\n",
    "\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    epoch: int\n",
    "    dropout_rng: Any\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=['eval'])\n",
    "def loss_fn(\n",
    "    params,\n",
    "    state: TrainState,\n",
    "    x_init: jnp.ndarray,\n",
    "    x: jnp.ndarray,\n",
    "    a_true: jnp.ndarray,\n",
    "    r_true: jnp.ndarray,\n",
    "    dropout_rng,\n",
    "    eval: bool\n",
    ") -> tuple[jnp.ndarray, tuple[jnp.ndarray, jnp.ndarray]]:\n",
    "    a = state.apply_fn({'params': params}, x_init, x, eval=eval, rngs={'dropout': dropout_rng})\n",
    "    loss, losses = calc_loss(x, a, a_true)\n",
    "\n",
    "    return loss, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from network.checkpoints import CheckpointManager, Checkpoint\n",
    "\n",
    "\n",
    "class MinibatchProducer:\n",
    "    def num_minibatch(self, num_batches: int) -> int:\n",
    "        pass\n",
    "\n",
    "    def next_minibatch(self, step: int) -> jnp.ndarray:\n",
    "        pass\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class MinibatchProducerSimple(MinibatchProducer):\n",
    "    batch_size: int\n",
    "\n",
    "    def num_minibatch(self, num_batches: int) -> int:\n",
    "        return num_batches // self.batch_size\n",
    "\n",
    "    def next_minibatch(self, step: int) -> jnp.ndarray:\n",
    "        return jnp.arange(self.batch_size) + (self.batch_size * step)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=['eval'])\n",
    "def train_step(\n",
    "    state: TrainState, x_init: jnp.ndarray, x: jnp.ndarray, a: jnp.ndarray, r: jnp.ndarray, eval: bool\n",
    ") -> tuple[TrainState, jnp.ndarray, jnp.ndarray]:\n",
    "\n",
    "    if not eval:\n",
    "        (loss, losses), grads = jax.value_and_grad(loss_fn, has_aux=True)(\n",
    "            state.params, state, x_init, x, a, r, state.dropout_rng, eval=eval\n",
    "        )\n",
    "        state = state.apply_gradients(grads=grads, dropout_rng=random.PRNGKey(state.epoch))\n",
    "    else:\n",
    "        loss, losses = loss_fn(\n",
    "            state.params, state, x_init, x, a, r, state.dropout_rng, eval=eval\n",
    "        )\n",
    "        state = state\n",
    "\n",
    "    return state, loss, losses\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    state: TrainState,\n",
    "    batches: list[jnp.ndarray],\n",
    "    minibatch_producer: MinibatchProducer,\n",
    "    eval: bool\n",
    "):\n",
    "    losses_history = []\n",
    "\n",
    "    num_steps = minibatch_producer.num_minibatch(len(batches))\n",
    "\n",
    "    with tqdm(range(num_steps)) as pbar:\n",
    "        for i in pbar:\n",
    "            indices = minibatch_producer.next_minibatch(i)\n",
    "\n",
    "            minibatch = [batch[indices] for batch in batches]\n",
    "\n",
    "            state, loss, losses = train_step(*minibatch, eval)\n",
    "            losses_history.append(jax.device_get(losses))\n",
    "\n",
    "            pbar.set_postfix({\"loss\": f\"{float(loss):.3f}\"})\n",
    "\n",
    "    return state, jnp.mean(jnp.array(losses_history), axis=0)\n",
    "\n",
    "\n",
    "def fit(\n",
    "    state: TrainState,\n",
    "    model_config: TransformerConfig,\n",
    "    checkpoint_manager: CheckpointManager,\n",
    "    train_batches: jnp.ndarray,\n",
    "    test_batches: jnp.ndarray,\n",
    "    minibatch_producer: MinibatchProducer,\n",
    "    epochs: int,\n",
    "):\n",
    "    for epoch in range(state.epoch + 1, state.epoch + 1 + epochs):\n",
    "        state, losses_train = train_epoch(\n",
    "            state, train_batches, minibatch_producer, eval=False\n",
    "        )\n",
    "        _, losses_test = train_epoch(\n",
    "            state, test_batches, minibatch_producer, eval=True\n",
    "        )\n",
    "\n",
    "        msg = f'Epoch: {epoch}, Loss: ({losses_train.sum():.3f}, {losses_test.sum():.3f})'\n",
    "\n",
    "        for i, name in enumerate(state.get_head_names()):\n",
    "            msg += f', {name}: ({losses_train[i]:.3f}, {losses_test[i]:.3f})'\n",
    "\n",
    "        print(msg)\n",
    "\n",
    "        state = state.replace(epoch=state.epoch + 1)\n",
    "        checkpoint_manager.save(Checkpoint(int(state.epoch), model_config, state.params))\n",
    "\n",
    "    return state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geister12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
