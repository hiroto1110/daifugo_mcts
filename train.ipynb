{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from functools import partial\n",
    "import dataclasses\n",
    "import serde\n",
    "\n",
    "import jax\n",
    "import optax\n",
    "from jax import random, numpy as jnp\n",
    "from flax import linen as nn\n",
    "\n",
    "\n",
    "@serde.serde\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    num_heads: int\n",
    "    embed_dim: int\n",
    "    num_hidden_layers: int\n",
    "    max_n_ply: int = 201\n",
    "    strategy: bool = False\n",
    "\n",
    "    def create_model(self) -> 'Transformer':\n",
    "        return Transformer(self)\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    embed_dim: int\n",
    "    vocab_sizes: list[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, tokens: jnp.ndarray, eval: bool):\n",
    "        embeddings = jnp.zeros((*tokens.shape[:-1], self.embed_dim))\n",
    "\n",
    "        for i in range(len(self.vocab_sizes)):\n",
    "            tokens_i = jnp.clip(tokens[..., i], 0, self.vocab_sizes[i] - 1)\n",
    "            embeddings += nn.Embed(self.vocab_sizes[i], self.embed_dim)(tokens_i)\n",
    "\n",
    "        embeddings = nn.LayerNorm(epsilon=1e-12)(embeddings)\n",
    "        embeddings = nn.Dropout(0.5, deterministic=eval)(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    num_heads: int\n",
    "    embed_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, mask):\n",
    "        seq_len = x.shape[1]\n",
    "        head_dim = self.embed_dim // self.num_heads\n",
    "\n",
    "        v = nn.Dense(features=self.embed_dim)(x)  # [Batch, SeqLen, Head * Dim]\n",
    "        q = nn.Dense(features=self.embed_dim)(x)  # [Batch, SeqLen, Head * Dim]\n",
    "        k = nn.Dense(features=self.embed_dim)(x)  # [Batch, SeqLen, Head * Dim]\n",
    "\n",
    "        v = v.reshape(-1, seq_len, self.num_heads, head_dim)  # [Batch, SeqLen, Head, Dim]\n",
    "        q = q.reshape(-1, seq_len, self.num_heads, head_dim)  # [Batch, SeqLen, Head, Dim]\n",
    "        k = k.reshape(-1, seq_len, self.num_heads, head_dim)  # [Batch, SeqLen, Head, Dim]\n",
    "\n",
    "        # [Batch, Head, SeqLen, SeqLen]\n",
    "        attention = (jnp.einsum('...qhd,...khd->...hqk', q, k) / jnp.sqrt(head_dim))\n",
    "\n",
    "        attention = jnp.where(mask, attention, -jnp.inf)\n",
    "        attention = nn.softmax(attention, axis=-1)\n",
    "\n",
    "        values = jnp.einsum('...hqk,...khd->...qhd', attention, v)  # [Batch, SeqLen, Head, Dim]\n",
    "        values = values.reshape(-1, seq_len, self.num_heads * head_dim)  # [Batch, SeqLen, Head Ã— Dim (=EmbedDim)]\n",
    "        out = nn.Dense(self.embed_dim)(values)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    embed_dim: int\n",
    "    intermediate_size: int = 128\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, eval):\n",
    "        x = nn.Dense(features=self.embed_dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.embed_dim)(x)\n",
    "        x = nn.Dropout(0.1, deterministic=eval)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    num_heads: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.attention = MultiHeadAttention(self.num_heads, self.embed_dim)\n",
    "        self.feed_forward = FeedForward(embed_dim=self.embed_dim)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, attention_mask, eval):\n",
    "        out = self.attention(x, attention_mask)\n",
    "\n",
    "        x = x + out\n",
    "        x = nn.LayerNorm()(x)\n",
    "        x = x + self.feed_forward(x, eval)\n",
    "        x = nn.LayerNorm()(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    config: TransformerConfig\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.config)\n",
    "\n",
    "    def setup(self):\n",
    "        self.embeddings = Embeddings(self.config.embed_dim, self.config.vocab_sizes)\n",
    "        self.st_dence = nn.Dense(features=self.config.embed_dim)\n",
    "\n",
    "        self.layers = [\n",
    "            TransformerBlock(self.config.num_heads, self.config.embed_dim)\n",
    "            for _ in range(self.config.num_hidden_layers)\n",
    "        ]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray, eval=True):\n",
    "        x = self.embeddings(x, eval)\n",
    "\n",
    "        # [Batch, 1, SeqLen, SeqLen]\n",
    "        mask = nn.make_causal_mask(jnp.zeros((x.shape[0], x.shape[1])), dtype=bool)\n",
    "\n",
    "        for i in range(self.config.num_hidden_layers):\n",
    "            x = self.layers[i](x, mask, eval=eval)\n",
    "\n",
    "        x = nn.Dropout(0.1, deterministic=eval)(x)\n",
    "\n",
    "        p = nn.Dense(features=32, name=\"head_p\")(x)\n",
    "        v = nn.Dense(features=7, name=\"head_v\")(x)\n",
    "        c = nn.Dense(features=8, name=\"head_c\")(x)\n",
    "\n",
    "        return p, v, c  # [Batch, SeqLen, ...]\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def calc_loss(\n",
    "    x: jnp.ndarray,\n",
    "    p_pred: jnp.ndarray, v_pred: jnp.ndarray, c_pred: jnp.ndarray,\n",
    "    p_true: jnp.ndarray, v_true: jnp.ndarray, c_true: jnp.ndarray,\n",
    ") -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "\n",
    "    mask = jnp.any(x != 0, axis=-1)\n",
    "    mask = mask.reshape(-1)\n",
    "\n",
    "    # [Batch, SeqLen, 144]\n",
    "    p_true = p_true.reshape(-1)\n",
    "    v_true = jnp.stack([v_true]*v_pred.shape[-2], axis=-1).reshape(-1)\n",
    "    c_true = jnp.stack([c_true]*c_pred.shape[-2], axis=-1).reshape(-1, 8)\n",
    "    # c_true = c_true.reshape(-1, 1, 8)\n",
    "\n",
    "    p_pred = p_pred.reshape(-1, 32)\n",
    "    v_pred = v_pred.reshape(-1, 7)\n",
    "    c_pred = c_pred.reshape(-1, 8)\n",
    "\n",
    "    loss_p = optax.softmax_cross_entropy_with_integer_labels(p_pred, p_true)\n",
    "    loss_v = optax.softmax_cross_entropy_with_integer_labels(v_pred, v_true)\n",
    "    loss_c = optax.sigmoid_binary_cross_entropy(c_pred, c_true).mean(axis=-1)\n",
    "\n",
    "    loss_p = jnp.average(loss_p, weights=mask)\n",
    "    loss_v = jnp.average(loss_v, weights=mask)\n",
    "    loss_c = jnp.average(loss_c, weights=mask)\n",
    "\n",
    "    loss = loss_p + loss_v + loss_c\n",
    "    losses = jnp.array([loss_p, loss_v, loss_c])\n",
    "\n",
    "    return loss, losses\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=['eval'])\n",
    "def loss_fn(\n",
    "    params,\n",
    "    state: TrainStateTransformer,\n",
    "    x: jnp.ndarray, st: jnp.ndarray,\n",
    "    p_true: jnp.ndarray,\n",
    "    v_true: jnp.ndarray,\n",
    "    c_true: jnp.ndarray,\n",
    "    dropout_rng,\n",
    "    eval: bool\n",
    ") -> tuple[jnp.ndarray, tuple[jnp.ndarray, jnp.ndarray]]:\n",
    "    # p, v, c = state.apply_fn({'params': params}, tokens, eval=eval, rngs={'dropout': dropout_rng})\n",
    "    p, v, c = state.apply_fn({'params': params}, x, st, eval=eval, rngs={'dropout': dropout_rng})\n",
    "    loss, losses = calc_loss(x, p, v, c, p_true, v_true, c_true)\n",
    "\n",
    "    return loss, losses\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geister12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
